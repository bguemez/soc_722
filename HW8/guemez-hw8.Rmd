---
title: "Exercises from Chpater 6 - Bayes Rules"
author: "Braulio Güémez"
date: "12/10/2021"
output:
  pdf_document: default
  html_document: default
---
```{r}
# Load packages
library(tidyverse)
library(janitor)
library(rstan)
library(bayesplot)
library(bayesrules)
```





## 6.1


a.

Step 1: Define a grid with numbers that correspond to the desired parameter  to estimate . The more numbers (pi, lambda, mu), the more defined our grid will be.

Step 2: Estimate the prior and the likelihood values at each of the values. 

Step 3: Approximate the posterior by multiplying the likelihood values and the prior values at each of  values from the grid. 

Step 4: Sample values from the approximated normal posterior probabilities 

b. 
In step 1, I would include a larger vector of numbers to have as many bins as possible to have a closer approximation to the posterior.

In step 4, I could also take a larger sample to have a robust estimation of the posterior probabilities.

  In sum, the more information we plug in the model, the most robust posterior approximation

## 6.2
```{r}
knitr::include_graphics("C:/Users/bguemez/OneDrive - Duke University/1. First Year/3. Stats/R_exercises/soc_722/HW8/trace_plots.png")

```


## 6.3

a. The chain is mixing too slowly.

The posterior approximation may not capture the total plausible values of the real posterior. 

b. The chain has high correlation.

This implies that the sample is not behaving as constituted by independent observations which would undermine the validity of the estimated posterior.


c. The chain has a tendency to get “stuck.”

The approximated posterior will over sample the values where the chain got stuck.

## 6.4

a. 
MCMC diagnostics are important because we don't have the real posterior model to compare it with the simulation! The only thing we have to estimate a decent approximate posterior model are those techniques. There is not another way.

b.
Because we can't calculate a posterior model by hand when we have multiple parameters that come from different data sources. 

c.
Rstan permits us to work in R but using Stan notation. Stan is particularly useful because it is flexible in the many assumptions of the models. In the chapter, we learn that we can plug in the data, the parameter(s) and the type of model involved in the simulation of the posterior. All in one step! That is very convenient.

d.
I still don't understand the general logic of how multiple parameters can be plugged in the posterior simulation.

## 6.5


```{r}
# Load packages
library(tidyverse)
library(janitor)
library(rstan)
library(bayesplot)
```

a. 
```{r}
# Step 1: Define a grid 
grid_data  <- data.frame(pi_grid = c(0,.25,.5,.75,1)) 

# Step 2: Evaluate the prior & likelihood at each pi
grid_data <- grid_data %>% 
  mutate(prior = dbeta(pi_grid, 3, 8),
         likelihood = dbinom(2, 10, pi_grid))

# Step 3: Approximate the posterior
grid_data <- grid_data %>% 
  mutate(unnormalized = likelihood * prior,
         posterior = unnormalized / sum(unnormalized))

grid_data |> ggplot(aes(pi_grid,posterior)) + geom_point() + geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior))

```


The values are _very_ concentrated in pi=.25, we need a larger grid!

b. 

```{r}
# Step 1: Define a grid 
grid_data  <- data.frame(pi_grid = seq(0,1,length=201)) 

# Step 2: Evaluate the prior & likelihood at each pi
grid_data <- grid_data %>% 
  mutate(prior = dbeta(pi_grid, 3, 8),
         likelihood = dbinom(2, 10, pi_grid))

# Step 3: Approximate the posterior
grid_data <- grid_data %>% 
  mutate(unnormalized = likelihood * prior,
         posterior = unnormalized / sum(unnormalized))

grid_data |> ggplot(aes(pi_grid,posterior)) + geom_point() + geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) + theme_minimal()
```


## 6.6 

```{r}
# Step 1: Define a grid of lambda values
grid_data   <- data.frame(
  lambda_grid = seq(from = 0, to = 8))

# Step 2: Evaluate the prior & likelihood at each lambda
grid_data <- grid_data %>% 
  mutate(prior = dgamma(lambda_grid, 20,5),
      likelihood = dpois(0, lambda_grid) * dpois(1, lambda_grid)*dpois(0,lambda_grid))

# Step 3: Approximate the posterior
grid_data <- grid_data %>% 
  mutate(unnormalized = likelihood*prior,
      posterior = unnormalized/sum(unnormalized))

# Set the seed
set.seed(84735)

# Step 4: sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

# Histogram of the grid simulation with posterior pdf 
ggplot(post_sample, aes(x = lambda_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dgamma, args = list(20+1, 5+3)) + 
  lims(x = c(0, 8))


```

```{r}

# Step 1: Define a grid of lambda values
grid_data   <- data.frame(
  lambda_grid = seq(from = 0, to = 8, length=201))

# Step 2: Evaluate the prior & likelihood at each lambda
grid_data <- grid_data %>% 
  mutate(prior = dgamma(lambda_grid, 20,5),
      likelihood = dpois(0, lambda_grid) * dpois(1, lambda_grid)*dpois(0,lambda_grid))

# Step 3: Approximate the posterior
grid_data <- grid_data %>% 
  mutate(unnormalized = likelihood*prior,
      posterior = unnormalized/sum(unnormalized))

# Set the seed
set.seed(84735)

# Step 4: sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

# Histogram of the grid simulation with posterior pdf 
ggplot(post_sample, aes(x = lambda_grid)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dgamma, args = list(20+1, 5+3)) + 
  lims(x = c(0, 8))


```


## 6.7 

a.

```{r}

dta <- c(7.1,8.9,8.4,8.6)

# Step 1: Define a grid 
grid_data  <- data.frame(m_grid = c(5:15)) 

# Step 2: Evaluate the prior & likelihood at each pi
grid_data <- grid_data %>% 
  mutate(prior = dnorm(m_grid, 10, 1.2),
         likelihood = exp(-((mean(dta)-10)^2/(2*(1.3^2/4)))))     

# Step 3: Approximate the posterior
grid_data <- grid_data %>% 
  mutate(unnormalized = likelihood * prior,
         posterior = unnormalized / sum(unnormalized))


grid_data |> ggplot(aes(m_grid,posterior)) + geom_point() + geom_segment(aes(x = m_grid, xend = m_grid, y = 0, yend = posterior)) + theme_minimal()
```

b.



```{r}

dta <- c(7.1,8.9,8.4,8.6)

# Step 1: Define a grid 
grid_data  <- data.frame(m_grid=seq(5,15,length=201)) 

# Step 2: Evaluate the prior & likelihood at each pi
grid_data <- grid_data %>% 
  mutate(prior = dnorm(m_grid, 10, 1.2),
         likelihood = exp(-((mean(dta)-10)^2/(2*(1.3^2/4)))))     

# Step 3: Approximate the posterior
grid_data <- grid_data %>% 
  mutate(unnormalized = likelihood * prior,
         posterior = unnormalized / sum(unnormalized))

grid_data |> ggplot(aes(m_grid,posterior)) + geom_point() + geom_segment(aes(x = m_grid, xend = m_grid, y = 0, yend = posterior)) + theme_minimal()
```





## 6.8

a.You might want to estimate consumer preference for certain product in different places where data about it has been collected. In this case, there are multiple parameters theta that can't be incorporated in a single grid approximation.

b. The curse of dimensionality means that the grid approximation with multiple parameters becomes computationally expensive. That is why MCMC approaches are a flexible alternative.

## 6.9. 

a.

Both samples are not taken directly from the posterior pdf. They are approximations.

b

Both are useful to approximate posteriors using one parameter.

c.

The obtained samples are independent and they are easier to understand than MCMC.

d.

You can calculate posteriors that involve multiple parameters and it is not that computationally expensive.

## 6.10

a. It is a Markov Chain because the decision of whether you go to a Thai restaurant depends on what restaurant you went to the day before.

b. The events are independent. This is not a Markov Chain. 

c. This may be a Markov Chain because, depending on their level of expertise, you can "learn" how your roommate plays and increase the probability of winning.


## 6.11

a.
```{r}


# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 20> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(20, pi);
    pi ~ beta(1, 1);
  }
"

```


b. I'll assume the vector of counts is 1 because it was not provided in the exercise 
```{r}

# STEP 1: DEFINE the model
gp_model <- "
  data {
    int<lower=0> Y[1];
  }
  parameters {
    real<lower=0> lambda;
  }
  model {
    Y ~ poisson(lambda);
    lambda ~ gamma(4, 2);
  }
"


```


c. I'll assume the vector of y is = 1 because it was not provided in the exercise 

```{r}
n_model <- '
data {
    vector[1] Y;  
} 
parameters {
    real mu;
} 
model {
   Y ~ normal(mu, 1);
   mu ~ normal(0, 10);
}
'

```


## 6.12

a.
```{r}


# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 20> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(20, pi);
    pi ~ beta(1, 1);
  }
"
# STEP 2: SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, data = list(Y = 12), 
               chains = 4, iter = 5000*2, seed = 84735)



```


b.
```{r}

# STEP 1: DEFINE the model
gp_model <- "
  data {
    int<lower=0> Y[1];
  }
  parameters {
    real<lower=0> lambda;
  }
  model {
    Y ~ poisson(lambda);
    lambda ~ gamma(4, 2);
  }
"
## Step 2: Simulate the posterior

gp_sim <- stan(model_code = gp_model, data = list(Y = 3), 
               chains = 4, iter = 5000*2, seed = 84735)




```


c.

```{r}
n_model <- '
data {
    vector[1] Y;  
} 
parameters {
    real mu;
} 
model {
   Y ~ normal(mu, 1);
   mu ~ normal(0, 10);
}
'


gn_sim <- stan(model_code = n_model, data = list(Y=12.2), 
               chains = 4, iter = 5000*2, seed = 84735)
```


## 6.13

a.
```{r}

# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 10> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(10, pi);
    pi ~ beta(1, 1);
  }
"
# STEP 2: SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, data = list(Y = 2), 
               chains = 3, iter = 12000, seed = 84735)

```
b.

```{r}
mcmc_trace(bb_sim, pars = "pi")
```


c.
The range of values is from 0 to 6000 iterations. I set it up for 12,000 but because they remove the first half that count as "burn-in" or "warm-up" samples, only 6000 are shown in the plot.


d.
```{r}
mcmc_dens_overlay(bb_sim, pars = "pi") + 
  ylab("density")
```

e. Both posteriors look identical!

```{r}
plot_beta_binomial(1,1,2,10)
```

## 6.14

```{r}

# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 12> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(12, pi);
    pi ~ beta(4, 3);
  }
"
# STEP 2: SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, data = list(Y = 4), 
               chains = 3, iter = 12000, seed = 84735)
```


b.
```{r}
mcmc_trace(bb_sim, pars = "pi")
```


c. The range of values is from 0 to 6000 iterations. I set it up for 12,000 but because they remove the first half that count as "burn-in" or "warm-up" samples, only 6000 are shown in the plot.


d.
```{r}
mcmc_dens_overlay(bb_sim, pars = "pi") + 
  ylab("density")
```

e. The real posterior and the approximate posterior look rather similar.
```{r}

plot_beta_binomial(4,3,4,12)

```


## Exercise 6.15

a.
```{r}

# STEP 1: DEFINE the model
gp_model <- "
  data {
    int<lower=0> Y[3];
  }
  parameters {
    real<lower=0> lambda;
  }
  model {
    Y ~ poisson(lambda);
    lambda ~ gamma(20,5);
  }
"

# Step 2: Approximate the posterior

gp_sim <- stan(model_code = gp_model, data = list(Y=c(0,1,0)), 
               chains = 4, iter = 5000*2, seed = 84735)
```


b.
```{r}
mcmc_trace(gp_sim,pars="lambda")

mcmc_dens_chains(gp_sim,pars="lambda")

```

c.
2.5 seems the most likely value

d.

The posterior mode is 2.6!
```{r}
summarize_gamma_poisson(20,5,1,3)
```



## Exercise 6.16

a.
```{r}

# STEP 1: DEFINE the model
gp_model <- "
  data {
    int<lower=0> Y[3];
  }
  parameters {
    real<lower=0> lambda;
  }
  model {
    Y ~ poisson(lambda);
    lambda ~ gamma(5,5);
  }
"

# Step 2: Approximate the posterior

gp_sim <- stan(model_code = gp_model, data = list(Y=c(0,1,0)), 
               chains = 4, iter = 5000*2, seed = 84735)
```


b.
```{r}
mcmc_trace(gp_sim,pars="lambda")

mcmc_dens_chains(gp_sim,pars="lambda")

```

c.
In this model, the most posterior plausible value seems to be .6

d.

The mode is exactly .6! 
```{r}
summarize_gamma_poisson(5,5,1,3)
```


## 6.17

```{r}

n_model <- '
data {
    vector[4] Y;  
} 
parameters {
    real mu;
} 
model {
   Y ~ normal(mu, 1.3);
   mu ~ normal(10,1.2);
}
'


gn_sim <- stan(model_code = n_model, data = list(Y = c(7.1, 8.9, 8.4, 8.6)), 
               chains = 4, iter = 5000*2, seed = 568956)

```


b.
```{r}
mcmc_trace(gn_sim,pars="mu")

mcmc_dens_chains(gn_sim,pars="mu")

```

c.
In this model, the most posterior plausible value seems to be 8.5

d. The mode is 8.6 in the real posterior, which matches our finding above.
```{r}
summarize_normal_normal(mean = 10,sd = 1.2,sigma = 1.3,y_bar = mean(c(7.1, 8.9, 8.4, 8.6)), n = 4) 
```



## 6.18

```{r}

n_model <- '
data {
    vector[5] Y;  
} 
parameters {
    real mu;
} 
model {
   Y ~ normal(mu, 8);
   mu ~ normal(-14,2);
}
'


gn_sim <- stan(model_code = n_model, data = list(Y = c(-10.1,5.5,.1,-1.4,11.5)), 
               chains = 4, iter = 5000*2, seed = 568956)

```





b.
```{r}
mcmc_trace(gn_sim,pars="mu")

mcmc_dens_chains(gn_sim,pars="mu")

```




c.
In this model, the most posterior plausible value seems to be -10

d. The mode is -10 in the real posterior, which matches our finding above!!
```{r}
summarize_normal_normal(mean = -14,sd = 2,sigma = 8,y_bar = mean(c(-10.1,5.5,.1,-1.4,11.5)), n = 5) 
```
